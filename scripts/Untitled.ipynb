{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "52f67ce0-0d34-4531-b88c-b285e50e9add",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nsimulation_data = test_data_copy  # Assuming this is defined with your actual data\\nsimulation_data.index = simulation_data.index.tz_localize(None)  # Remove timezone information\\nstart_date = '2022-05-20'\\nend_date = '2024-03-20'\\n\\nsimulation = RL_VaultSimulator(simulation_data, simulation_data, features, targets, temporals, start_date, end_date, scale_factor=300000000, minimum_value_factor=0.05, volatility_window=250)\\nsimulation.train_model()\\nsimulation.run_simulation(start_date)\\nsimulation.plot_simulation_results()\\n\\n\\n# Plot Dai ceilings and USD balances\\nvault_names = ['ETH', 'stETH', 'BTC', 'Altcoin', 'Stablecoin', 'LP', 'PSM', 'RWA']\\nsimulation.plot_dai_ceilings_and_usd_balances(start_date, vault_names)\\n\\n# Calculate error metrics against actual data (if available)\\n# simulation.calculate_error_metrics(actual_data)\\n\\n\\n# result = simulation.results\\n# evaluate_predictions(result, historical)\\n\\n# ### Filter for MVO\\n\\n# In[852]:\\n\\n\\nhistorical_data = historical[historical.index <= '2022-05-19']\\n\\n\\n# In[853]:\\n\\n\\nhistorical_data_mvo = historical_data.copy()\\n\\nhistorical_data_mvo.index= historical_data_mvo.index.tz_localize(None)\\n\\n\\n# In[854]:\\n\\n\\nresult.index\\n\\n\\n# In[855]:\\n\\n\\ncombined_data = pd.concat([historical_data_mvo, result])\\n\\n# Optional: Sort the DataFrame by index if it's not already sorted\\ncombined_data.sort_index(inplace=True)\\n\\n# Now 'combined_data' contains both historical and simulation data in one DataFrame\\nprint(combined_data)\\n\\n\\n# In[856]:\\n\\n\\nhistorical_comparison = historical[historical.index <= '2022-06-12']\\nhistorical_comparison\\n\\n\\n# In[857]:\\n\\n\\nresult\\n\""
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random\n",
    "import time\n",
    "from datetime import timedelta\n",
    "\n",
    "# Machine learning tools\n",
    "from sklearn.linear_model import LinearRegression, Ridge, MultiTaskLassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "# Deep Learning tools\n",
    "import tensorflow as tf\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Input\n",
    "from keras.optimizers import Adam, RMSprop, SGD\n",
    "from keras.callbacks import ReduceLROnPlateau, EarlyStopping\n",
    "from keras.regularizers import l2\n",
    "\n",
    "# Additional tools\n",
    "from scipy import signal\n",
    "from scipy.optimize import minimize\n",
    "from itertools import combinations, product\n",
    "\n",
    "# External data and APIs\n",
    "import yfinance as yf\n",
    "import requests\n",
    "import streamlit as st\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import ScalarFormatter\n",
    "import matplotlib as mpl\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objs as go\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)\n",
    "\n",
    "class RL_VaultSimulator:\n",
    "    def __init__(self, data, initial_data, features, targets, temporals, start_date, end_date, scale_factor=300000000, minimum_value_factor=0.05, volatility_window=250, alpha=100, lp_volatility_scale=1.0):\n",
    "        self.data = data[data.index <= pd.to_datetime(start_date).tz_localize(None)]\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.alpha = alpha\n",
    "        self.model = None\n",
    "        self.temporals = temporals\n",
    "        self.results = pd.DataFrame()\n",
    "        self.initial_data = initial_data\n",
    "        self.start_date = pd.to_datetime(start_date).tz_localize(None)\n",
    "        self.end_date = pd.to_datetime(end_date).tz_localize(None)\n",
    "        self.current_date = self.start_date\n",
    "        self.dai_ceilings_history = pd.DataFrame()\n",
    "        self.volatility_window = volatility_window\n",
    "        self.scale_factor = scale_factor\n",
    "        self.minimum_value_factor = minimum_value_factor\n",
    "        self.lp_volatility_scale = lp_volatility_scale  # Add new parameter\n",
    "\n",
    "\n",
    "    def get_latest_data(self):\n",
    "        # Return the latest data that the environment will use\n",
    "        return self.data\n",
    "        \n",
    "    def reset(self):\n",
    "        self.data = self.initial_data[self.initial_data.index <= self.start_date]\n",
    "        self.results = pd.DataFrame()\n",
    "        self.train_model()\n",
    "        print('sim reset current date', self.current_date)\n",
    "        print(\"Simulation reset and model retrained.\")\n",
    "\n",
    "    def train_model(self):\n",
    "        X = self.initial_data[self.features]\n",
    "        y = self.initial_data[self.targets]\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.8, random_state=42)\n",
    "        self.model = MultiOutputRegressor(Ridge(alpha=self.alpha))\n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(\"Model trained.\")\n",
    "        \n",
    "    def update_dai_ceilings(self):\n",
    "        # Extract the current DAI ceilings\n",
    "        current_ceilings = self.data.loc[self.current_date, ['ETH Vault_dai_ceiling', 'BTC Vault_dai_ceiling', 'stETH Vault_dai_ceiling', 'Altcoin Vault_dai_ceiling', 'Stablecoin Vault_dai_ceiling', 'LP Vault_dai_ceiling','RWA Vault_dai_ceiling','PSM Vault_dai_ceiling']]\n",
    "        current_ceilings['timestamp'] = self.current_date  # Adding a timestamp for reference\n",
    "\n",
    "        # Append to the historical DataFrame\n",
    "        self.dai_ceilings_history = pd.concat([self.dai_ceilings_history, current_ceilings.to_frame().T.set_index('timestamp')])\n",
    "\n",
    "\n",
    "\n",
    "    def run_simulation(self, simulation_date, action=None):\n",
    "        # Ensure the date is timezone-aware. Localize if it is naive.\n",
    "        #if pd.to_datetime(simulation_date).tzinfo is None:\n",
    "           # self.current_date = pd.to_datetime(simulation_date).tz_localize('UTC')\n",
    "        #else:\n",
    "            #self.current_date = pd.to_datetime(simulation_date).tz_convert('UTC')\n",
    "    \n",
    "        cycle_start_date = self.current_date\n",
    "        end_date = min(cycle_start_date + timedelta(days=24), self.end_date)\n",
    "    \n",
    "        while self.current_date <= end_date and self.current_date <= self.initial_data.index.max():\n",
    "            if self.current_date in self.data.index:\n",
    "                X_test = self.data.loc[[self.current_date], self.features]\n",
    "            else:\n",
    "                X_test = self.data.tail(1)[self.features]\n",
    "    \n",
    "            volatilities = self.calculate_historical_volatility()\n",
    "            predictions = self.forecast(X_test, volatilities)\n",
    "            print('predictions',predictions)\n",
    "            #print('predcitions nan', predictions.isna().sum().sum())\n",
    "            future_index = pd.DatetimeIndex([self.current_date])\n",
    "            print('future index', future_index)\n",
    "            self.update_state(future_index, predictions)\n",
    "    \n",
    "            # Update the DAI ceilings history right after updating the state\n",
    "            self.update_dai_ceilings()\n",
    "    \n",
    "            print('current state', self.data.iloc[-1])\n",
    "            if action:\n",
    "                self.apply_action(action)\n",
    "                print(f\"Action applied on: {self.current_date}\")\n",
    "    \n",
    "            print(f\"Day completed: {self.current_date}\")\n",
    "            self.current_date += timedelta(days=1)\n",
    "    \n",
    "        print(f\"Cycle completed up to: {self.current_date - timedelta(days=1)}\")\n",
    "        if self.current_date > self.end_date:\n",
    "            print(f\"Simulation completed up to: {self.end_date}\")\n",
    "        else:\n",
    "            print(f\"Simulation completed up to: {self.current_date - timedelta(days=1)}\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def apply_action(self, action):\n",
    "        base_value_if_zero = 10000  # Base value to set if the initial DAI ceiling is zero\n",
    "        if action:\n",
    "            for vault, percentage_change in action.items():\n",
    "                # Append the suffix '_dai_ceiling' to the vault name to match the DataFrame columns\n",
    "                dai_ceiling_key = vault.replace('_collateral_usd', '_dai_ceiling') \n",
    "                print('vault', vault)\n",
    "                print('Applying action to:', dai_ceiling_key)\n",
    "                if dai_ceiling_key in self.data.columns:\n",
    "                    original_value = self.data[dai_ceiling_key].iloc[-1]\n",
    "                    if original_value == 0:\n",
    "                        # If the original value is 0, initialize it with the base value\n",
    "                        new_value = base_value_if_zero * (1 + percentage_change / 100)\n",
    "                        print(f\"Initialized and adjusted {dai_ceiling_key} from 0 to {new_value}\")\n",
    "                    else:\n",
    "                        new_value = original_value * (1 + percentage_change / 100)\n",
    "                        print(f\"Adjusted {dai_ceiling_key} by {percentage_change}% from {original_value} to {new_value}\")\n",
    "\n",
    "                    self.data.at[self.data.index[-1], dai_ceiling_key] = new_value\n",
    "                else:\n",
    "                    print(f\"No 'dai_ceiling' column found for {dai_ceiling_key}, no action applied.\")\n",
    "        else:\n",
    "            print(\"No action provided; no adjustments made.\")\n",
    "\n",
    "\n",
    "    def forecast(self, X, volatilities):\n",
    "        predictions = self.model.predict(X)\n",
    "        predictions = np.maximum(predictions, 0)\n",
    "\n",
    "        scale_factor = self.scale_factor\n",
    "        noise = np.random.normal(0, volatilities * scale_factor, predictions.shape)\n",
    "        minimum_value = self.minimum_value_factor * self.initial_data[self.targets].mean()\n",
    "        adjusted_predictions = np.maximum(predictions + noise, minimum_value)\n",
    "\n",
    "        # Apply specific adjustments for the LP vault\n",
    "        lp_vault_index = self.targets.index('LP Vault_collateral_usd')\n",
    "        if lp_vault_index is not None:\n",
    "            lp_volatility_cap = volatilities[lp_vault_index] * scale_factor * self.lp_volatility_scale  # Use lp_volatility_scale\n",
    "            lp_noise = np.random.normal(0, lp_volatility_cap, adjusted_predictions[:, lp_vault_index].shape)\n",
    "            adjusted_predictions[:, lp_vault_index] = np.maximum(predictions[:, lp_vault_index] + lp_noise, minimum_value[lp_vault_index])\n",
    "\n",
    "        return adjusted_predictions\n",
    "\n",
    "\n",
    "        # try scale 453000000, min val .12 or .1, window 25 or 15\n",
    "\n",
    "\n",
    "    def calculate_historical_volatility(self):\n",
    "        window = self.volatility_window\n",
    "    \n",
    "        # Assuming daily data, calculate percentage change\n",
    "        daily_returns = self.data[self.targets].pct_change()\n",
    "    \n",
    "        # Handling possible NaN values in daily returns\n",
    "        daily_returns = daily_returns.dropna()\n",
    "    \n",
    "        # Calculate volatility as the standard deviation of returns\n",
    "        volatility = daily_returns.rolling(window=window, min_periods=1).std()\n",
    "        print('all vol before', volatility.describe())\n",
    "    \n",
    "        # Adjust volatility for the LP vault\n",
    "        lp_vault = 'LP Vault_collateral_usd'\n",
    "        print('lp vol before', volatility[lp_vault])\n",
    "        if lp_vault in volatility.columns:\n",
    "            # Apply the lp_volatility_scale factor for the LP vault\n",
    "            volatility[lp_vault] *= self.lp_volatility_scale\n",
    "            print('lp vol after', volatility[lp_vault])\n",
    "    \n",
    "        print('all vol after', volatility.describe())\n",
    "    \n",
    "        # Return the average volatility over the window\n",
    "        return volatility.mean(axis=0)  # Use axis=0 to average volatilities across columns if neededis=0 to average volatilities across columns if needed\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def update_state(self, indices, predictions):\n",
    "        #print('Current state', self.data[self.targets].iloc[-1])\n",
    "        #print('Current temporal', self.data[self.temporals].iloc[-1])\n",
    "        # Create a new DataFrame for the predictions\n",
    "        new_data = pd.DataFrame(predictions, index=indices, columns=self.targets)\n",
    "        new_data = new_data.clip(lower=0)\n",
    "        #print('New data', new_data)\n",
    "        self.results = pd.concat([self.results, new_data])  # Append new data to results\n",
    "        self.data.update(new_data)\n",
    "        # Append new data if the index does not already exist\n",
    "        if not self.data.index.isin(indices).any():\n",
    "            # Directly assign the new data to the respective index positions\n",
    "            self.data = self.data.reindex(self.data.index.union(new_data.index), method='nearest')\n",
    "            for column in new_data.columns:\n",
    "                self.data.loc[new_data.index, column] = new_data[column]\n",
    "            self.data.sort_index(inplace=True)  # Ensure the index is sorted\n",
    "        else:\n",
    "            # If indices overlap, directly update the values\n",
    "            self.data.update(new_data)\n",
    "        #print('new state update:',self.data[self.targets].iloc[-1],self.data[self.temporals].iloc[-1])\n",
    "    \n",
    "        # Recalculate temporal features right after updating the state\n",
    "        self.recalculate_temporal_features(indices)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    def recalculate_temporal_features(self, start_index):\n",
    "        vault_names = ['ETH Vault', 'stETH Vault', 'BTC Vault', 'Altcoin Vault', 'Stablecoin Vault', 'LP Vault', 'PSM Vault', 'RWA Vault']\n",
    "        total_usd_col = 'Vaults Total USD Value'\n",
    "        self.data[total_usd_col] = self.data[[f'{vault}_collateral_usd' for vault in vault_names]].sum(axis=1)\n",
    "    \n",
    "        for vault in vault_names:\n",
    "            usd_col = f'{vault}_collateral_usd'\n",
    "            pct_col = f'{vault}_collateral_usd % of Total'\n",
    "            self.data[pct_col] = self.data[usd_col] / self.data[total_usd_col]  # Update the percentage column\n",
    "            # Calculate the 7-day moving average for the USD collateral\n",
    "            #ma_col_usd_7d = f'{usd_col}_7d_ma'\n",
    "            #self.data[ma_col_usd_7d] = self.data[usd_col].rolling(window=7, min_periods=1).mean()\n",
    "            ma_col_usd_30d = f'{usd_col}_30d_ma'\n",
    "            self.data[ma_col_usd_30d] = self.data[usd_col].rolling(window=30, min_periods=1).mean()\n",
    "            # Calculate the 7-day and 30-day moving averages for the percentage of total\n",
    "            for window in [7,30]:\n",
    "                ma_col_pct = f'{pct_col}_{window}d_ma'\n",
    "                self.data[ma_col_pct] = self.data[pct_col].rolling(window=window, min_periods=1).mean()\n",
    "             \n",
    "            dai_ceiling_col = f'{vault}_dai_ceiling'\n",
    "            if dai_ceiling_col in self.data.columns:\n",
    "                prev_dai_ceiling_col = f'{vault}_prev_dai_ceiling'\n",
    "                self.data[prev_dai_ceiling_col] = self.data[dai_ceiling_col].shift(1)\n",
    "    def print_summary_statistics(self, pre_data):\n",
    "        for column in self.data.columns:\n",
    "            if column not in pre_data.columns:\n",
    "                pre_data[column] = pd.NA  # Handle missing column in pre_data\n",
    "            pre_stats = pre_data.describe()\n",
    "            post_stats = self.data.describe()\n",
    "            print(f\"--- {column} ---\")\n",
    "            print(\"Pre-Simulation:\\n\", pre_stats)\n",
    "            print(\"Post-Simulation:\\n\", post_stats, \"\\n\")\n",
    "            \n",
    "    def plot_vault_data(self, column):\n",
    "        vault_usd_col = f'{column}_collateral_usd'\n",
    "        vault_dai_col = f'{column}_dai_ceiling'\n",
    "        \n",
    "        fig = sp.make_subplots(specs=[[{\"secondary_y\": True}]])\n",
    "        \n",
    "        # Plot USD balance on the primary y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=self.data.index, y=self.data[vault_usd_col], name=f'{column} USD Balance', line=dict(color='blue')),\n",
    "            secondary_y=False\n",
    "        )\n",
    "        \n",
    "        # Plot DAI ceiling on the secondary y-axis\n",
    "        fig.add_trace(\n",
    "            go.Scatter(x=self.data.index, y=self.data[vault_dai_col], name=f'{column} DAI Ceiling', line=dict(color='orange')),\n",
    "            secondary_y=True\n",
    "        )\n",
    "        \n",
    "        # Set x-axis title\n",
    "        fig.update_xaxes(title_text=\"Date\")\n",
    "        \n",
    "        # Set y-axes titles\n",
    "        fig.update_yaxes(title_text=\"USD Balance\", secondary_y=False, tickcolor='blue')\n",
    "        fig.update_yaxes(title_text=\"DAI Ceiling\", secondary_y=True, tickcolor='orange')\n",
    "        \n",
    "        # Update layout\n",
    "        fig.update_layout(\n",
    "            title_text=f\"Time Series for {column}\",\n",
    "            legend=dict(x=0.01, y=0.99, bordercolor=\"Black\", borderwidth=1)\n",
    "        )\n",
    "        \n",
    "        return fig\n",
    "        \n",
    "        \n",
    "    def plot_simulation_results(self):\n",
    "        fig, ax = plt.subplots(figsize=(14, 7))\n",
    "        for target in self.targets:\n",
    "            ax.plot(self.results.index, self.results[target], label=target)\n",
    "        ax.set_title(\"Simulation Results\")\n",
    "        ax.set_xlabel('Date')\n",
    "        ax.set_ylabel('Values')\n",
    "        ax.legend()\n",
    "        ax.grid(True)\n",
    "        #ax.ticklabel_format(useOffset=False, style='plain')\n",
    "        apply_scalar_formatter(ax)\n",
    "        plt.show()\n",
    "\n",
    "    def plot_dai_ceilings_and_usd_balances(self, start_simulation_date, vault_names):\n",
    "        if isinstance(start_simulation_date, str):\n",
    "            start_simulation_date = pd.to_datetime(start_simulation_date)\n",
    "        if not isinstance(self.data.index, pd.DatetimeIndex):\n",
    "            self.data.index = pd.to_datetime(self.data.index)\n",
    "        \n",
    "        fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(14, 10), sharex=True)\n",
    "        \n",
    "        for vault in vault_names:\n",
    "            axes[0].plot(self.data.index, self.data[f'{vault} Vault_dai_ceiling'], label=f'{vault} Dai Ceiling')\n",
    "        axes[0].axvline(x=start_simulation_date, color='r', linestyle='--', label='Start of Simulation')\n",
    "        axes[0].set_title('Dai Ceilings Over Time')\n",
    "        axes[0].set_ylabel('Dai Ceiling')\n",
    "        axes[0].legend()\n",
    "        #axes[0].ticklabel_format(useOffset=False, style='plain')\n",
    "        apply_scalar_formatter(axes[0])\n",
    "        \n",
    "        for vault in vault_names:\n",
    "            axes[1].plot(self.data.index, self.data[f'{vault} Vault_collateral_usd'], label=f'{vault} USD Balance')\n",
    "        axes[1].axvline(x=start_simulation_date, color='r', linestyle='--', label='Start of Simulation')\n",
    "        axes[1].set_title('USD Balances Per Vault Over Time')\n",
    "        axes[1].set_ylabel('USD Balance')\n",
    "        axes[1].set_xlabel('Date')\n",
    "        axes[1].legend()\n",
    "        #axes[1].ticklabel_format(useOffset=False, style='plain')\n",
    "        apply_scalar_formatter(axes[1])\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "    def calculate_error_metrics(self, actual_data):\n",
    "        vault_names = ['ETH', 'stETH', 'BTC', 'Altcoin', 'Stablecoin', 'LP', 'PSM']\n",
    "        for vault in vault_names:\n",
    "            column = f'{vault} Vault_collateral_usd'\n",
    "            try:\n",
    "                mse = mean_squared_error(actual_data[column], self.data[column])\n",
    "                mae = mean_absolute_error(actual_data[column], self.data[column])\n",
    "                rmse = sqrt(mse)\n",
    "                print(f\"--- Metrics for {vault} Vault ---\")\n",
    "                print(f\"MSE: {mse}\")\n",
    "                print(f\"MAE: {mae}\")\n",
    "                print(f\"RMSE: {rmse}\\n\")\n",
    "            except KeyError:\n",
    "                print(f\"Data for {vault} Vault not available in the dataset.\")\n",
    "\n",
    "\"\"\"\n",
    "simulation_data = test_data_copy  # Assuming this is defined with your actual data\n",
    "simulation_data.index = simulation_data.index.tz_localize(None)  # Remove timezone information\n",
    "start_date = '2022-05-20'\n",
    "end_date = '2024-03-20'\n",
    "\n",
    "simulation = RL_VaultSimulator(simulation_data, simulation_data, features, targets, temporals, start_date, end_date, scale_factor=300000000, minimum_value_factor=0.05, volatility_window=250)\n",
    "simulation.train_model()\n",
    "simulation.run_simulation(start_date)\n",
    "simulation.plot_simulation_results()\n",
    "\n",
    "\n",
    "# Plot Dai ceilings and USD balances\n",
    "vault_names = ['ETH', 'stETH', 'BTC', 'Altcoin', 'Stablecoin', 'LP', 'PSM', 'RWA']\n",
    "simulation.plot_dai_ceilings_and_usd_balances(start_date, vault_names)\n",
    "\n",
    "# Calculate error metrics against actual data (if available)\n",
    "# simulation.calculate_error_metrics(actual_data)\n",
    "\n",
    "\n",
    "# result = simulation.results\n",
    "# evaluate_predictions(result, historical)\n",
    "\n",
    "# ### Filter for MVO\n",
    "\n",
    "# In[852]:\n",
    "\n",
    "\n",
    "historical_data = historical[historical.index <= '2022-05-19']\n",
    "\n",
    "\n",
    "# In[853]:\n",
    "\n",
    "\n",
    "historical_data_mvo = historical_data.copy()\n",
    "\n",
    "historical_data_mvo.index= historical_data_mvo.index.tz_localize(None)\n",
    "\n",
    "\n",
    "# In[854]:\n",
    "\n",
    "\n",
    "result.index\n",
    "\n",
    "\n",
    "# In[855]:\n",
    "\n",
    "\n",
    "combined_data = pd.concat([historical_data_mvo, result])\n",
    "\n",
    "# Optional: Sort the DataFrame by index if it's not already sorted\n",
    "combined_data.sort_index(inplace=True)\n",
    "\n",
    "# Now 'combined_data' contains both historical and simulation data in one DataFrame\n",
    "print(combined_data)\n",
    "\n",
    "\n",
    "# In[856]:\n",
    "\n",
    "\n",
    "historical_comparison = historical[historical.index <= '2022-06-12']\n",
    "historical_comparison\n",
    "\n",
    "\n",
    "# In[857]:\n",
    "\n",
    "\n",
    "result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a193d8b7-3700-4ce2-9c7b-48994af7b97a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File does not exist at the specified path.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "file_path = 'E:\\Projects\\robo_advisor_app-Copy1\\data\\csv\\test_data.csv'\n",
    "\n",
    "# Check if the file exists\n",
    "if os.path.exists(file_path):\n",
    "    # Attempt to read the file\n",
    "    try:\n",
    "        test_data = pd.read_csv(file_path)\n",
    "        print(\"File read successfully.\")\n",
    "    except PermissionError as e:\n",
    "        print(f\"PermissionError: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "else:\n",
    "    print(\"File does not exist at the specified path.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba32e7d8-d6d2-4815-8aef-314ce1618ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "import numpy as np\n",
    "\n",
    "def tune_model_hyperparameters(simulator, param_grid):\n",
    "    X = simulator.initial_data[simulator.features]\n",
    "    y = simulator.initial_data[simulator.targets]\n",
    "\n",
    "    model = MultiOutputRegressor(Ridge())\n",
    "\n",
    "    # Define grid search\n",
    "    grid_search = GridSearchCV(model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Update simulator model with best parameters\n",
    "    simulator.model = grid_search.best_estimator_\n",
    "    simulator.alpha = grid_search.best_params_['estimator__alpha']\n",
    "\n",
    "    print(\"Best parameters found for the model: \", grid_search.best_params_)\n",
    "\n",
    "# Example usage\n",
    "model_param_grid = {\n",
    "    'estimator__alpha': [100, 300, 500],\n",
    "}\n",
    "\n",
    "simulation_data = test_data_copy  # Assuming this is defined with your actual data\n",
    "start_date = '2022-05-20'\n",
    "end_date = '2023-06-20'  # Define an end date\n",
    "\n",
    "simulation = RL_VaultSimulator(simulation_data, simulation_data, features, targets, temporals, start_date, end_date)\n",
    "simulation.train_model()\n",
    "tune_model_hyperparameters(simulation, model_param_grid)\n",
    "\n",
    "# Manually set simulator-specific parameters\n",
    "simulation.scale_factor = 300000000  # Example value\n",
    "simulation.minimum_value_factor = 0.05  # Example value\n",
    "simulation.volatility_window = 250  # Example value\n",
    "\n",
    "# Run the simulation\n",
    "simulation.run_simulation(start_date)\n",
    "simulation.plot_simulation_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c42df1-a0ee-46a4-85c8-d297e014ad29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_grid_search(simulator, param_grid):\n",
    "    kf = KFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    best_params = None\n",
    "    best_score = float('inf')\n",
    "\n",
    "    for scale_factor in param_grid['scale_factor']:\n",
    "        for minimum_value_factor in param_grid['minimum_value_factor']:\n",
    "            for window in param_grid['volatility_window']:\n",
    "                scores = []\n",
    "\n",
    "                for train_index, test_index in kf.split(simulator.initial_data):\n",
    "                    train_data = simulator.initial_data.iloc[train_index]\n",
    "                    test_data = simulator.initial_data.iloc[test_index]\n",
    "\n",
    "                    temp_simulator = RL_VaultSimulator(\n",
    "                        train_data,\n",
    "                        simulator.initial_data,\n",
    "                        simulator.features,\n",
    "                        simulator.targets,\n",
    "                        simulator.temporals,\n",
    "                        simulator.start_date,\n",
    "                        simulator.end_date,\n",
    "                        simulator.alpha\n",
    "                    )\n",
    "                    temp_simulator.scale_factor = scale_factor\n",
    "                    temp_simulator.minimum_value_factor = minimum_value_factor\n",
    "                    temp_simulator.volatility_window = window\n",
    "\n",
    "                    temp_simulator.train_model()\n",
    "                    temp_simulator.run_simulation(simulator.start_date)\n",
    "\n",
    "                    # Reindex results to match test_data index\n",
    "                    y_pred = temp_simulator.results.reindex(test_data.index)\n",
    "                    y_true = test_data[simulator.targets]\n",
    "\n",
    "                    # Drop rows with NaN values that might have been introduced by reindexing\n",
    "                    common_index = y_pred.dropna().index\n",
    "                    y_pred = y_pred.loc[common_index]\n",
    "                    y_true = y_true.loc[common_index]\n",
    "\n",
    "                    score = mean_squared_error(y_true, y_pred)\n",
    "                    scores.append(score)\n",
    "\n",
    "                mean_score = np.mean(scores)\n",
    "                if mean_score < best_score:\n",
    "                    best_score = mean_score\n",
    "                    best_params = {\n",
    "                        'scale_factor': scale_factor,\n",
    "                        'minimum_value_factor': minimum_value_factor,\n",
    "                        'volatility_window': window\n",
    "                    }\n",
    "\n",
    "    return best_params, best_score\n",
    "\n",
    "# Usage example\n",
    "simulation_data = test_data_copy\n",
    "if simulation_data.index.tz is not None:\n",
    "    simulation_data.index = simulation_data.index.tz_convert(None)\n",
    "else:\n",
    "    simulation_data.index = pd.to_datetime(simulation_data.index)\n",
    "\n",
    "# Now remove timezone information\n",
    "simulation_data.index = simulation_data.index.tz_localize(None)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'scale_factor': [10000000, 50000000, 100000000, 300000000, 453000000],\n",
    "    'minimum_value_factor': [0.001, 0.008,0.05, 0.1, 0.12],\n",
    "    'volatility_window': [60, 150, 180, 250, 280, 360, 380]\n",
    "}\n",
    "\n",
    "\n",
    "# Usage example\n",
    "start_date = '2022-05-20'\n",
    "end_date = '2024-03-20'\n",
    "\n",
    "simulation = VaultSimulator(simulation_data, simulation_data, features, targets, temporals, start_date, end_date)\n",
    "simulation.train_model()\n",
    "best_params, best_score = custom_grid_search(simulation, param_grid)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
